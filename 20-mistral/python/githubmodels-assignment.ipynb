{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building with Mistral Models \n",
    "\n",
    "## Introduction \n",
    "\n",
    "This lesson will cover: \n",
    "- Exploring the different Mistral Models \n",
    "- Understanding the use-cases and scenarios for each model \n",
    "- Code samples show the unique features of each model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mistral Models \n",
    "\n",
    "In this lesson, we will explore 3 different Mistral models: \n",
    "**Mistral Large**, **Mistral Small** and **Mistral Nemo**. \n",
    "\n",
    "Each of these models are available free on the Github Model marketplace. The code in this notebook will be using this models to run the code. Here are more details on using Github Models to [prototype with AI models](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large 2 (2407)\n",
    "Mistral Large 2 is currently the flagship model from Mistral and is designed for enterprise use. \n",
    "\n",
    "The model is an  upgrade to the original Mistral Large by offering \n",
    "-  Larger Context Window - 128k vs 32k \n",
    "-  Better performance on Math and Coding Tasks - 76.9% average accuracy vs 60.4% \n",
    "-  Increased multilingual performance - languages include: English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi.\n",
    "\n",
    "With these features, Mistral Large excels at \n",
    "- *Retrieval Augmented Generation (RAG)* - due to the larger context window\n",
    "- *Function Calling* - this model has native function calling which allows integration with external tools and APIs. These calls can be made both in parallel or one after another in a sequential order. \n",
    "- *Code Generation* - this model excels on Python, Java, TypeScript and C++ generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Example using Mistral Large 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are using Mistral Large 2 to run a RAG pattern over a text document. The question is written in Korean and asks about the author's activities before college. \n",
    "\n",
    "It uses Cohere Embeddings Model to create embeddings of the text document as well as the question. For this sample, it uses the faiss Python package as a vector store. \n",
    "\n",
    "The prompt sent to the Mistral model includes both the questions and the retrieved chunks that are similar to the question. The Model then provides a natural language response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-musllinux_1_2_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-musllinux_1_2_x86_64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two main activities that the author did before coming to the university are not known.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-nemo\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Small \n",
    "Mistral Small is another model in the Mistral family of models under the premier/enterprise category. As the name implies, this model is a Small Language Model (SLM). The advantages of using Mistral Small are that it is: \n",
    "- Cost Saving compared to Mistral LLMs like Mistral Large and NeMo - 80% price drop\n",
    "- Low latency - faster response compared to Mistral's LLMs\n",
    "- Flexible - can be deployed across different environments with less restrictions on required resources. \n",
    "\n",
    "\n",
    "Mistral Small is great for: \n",
    "- Text based tasks such as summarization, sentiment analysis and translation. \n",
    "- Applications where frequent requests are made due to its cost effectiveness \n",
    "- Low latency code tasks like review and code suggestions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Mistral Small and Mistral Large \n",
    "\n",
    "To show differences in latency between Mistral Small and Large, run the below cells. \n",
    "\n",
    "You should see a difference in response times between 3-5 seconds. Also not the response lengths and style over the smae prompt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! The Fizz Buzz test is a common programming task where the function prints the numbers from 1 to 100, but for multiples of three, it prints \"Fizz\" instead of the number, and for multiples of five, it prints \"Buzz.\" For numbers which are multiples of both three and five, it prints \"FizzBuzz.\"\n",
      "\n",
      "Here's the Python function you requested:\n",
      "\n",
      "```python\n",
      "def fizz_buzz():\n",
      "    for i in range(1, 101):\n",
      "        if i % 3 == 0 and i % 5 == 0:\n",
      "            print(\"FizzBuzz\", end=\" \")\n",
      "        elif i % 3 == 0:\n",
      "            print(\"Fizz\", end=\" \")\n",
      "        elif i % 5 == 0:\n",
      "            print(\"Buzz\", end=\" \")\n",
      "        else:\n",
      "            print(i, end=\" \")\n",
      "    print()\n",
      "\n",
      "fizz_buzz()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a simple Python function that implements the FizzBuzz test. This function prints the FizzBuzz sequence for the first `n` numbers.\n",
      "\n",
      "```python\n",
      "def fizz_buzz(n):\n",
      "    for i in range(1, n + 1):\n",
      "        if i % 3 == 0 and i % 5 == 0:\n",
      "            print(\"FizzBuzz\")\n",
      "        elif i % 3 == 0:\n",
      "            print(\"Fizz\")\n",
      "        elif i % 5 == 0:\n",
      "            print(\"Buzz\")\n",
      "        else:\n",
      "            print(i)\n",
      "\n",
      "# Test the function\n",
      "fizz_buzz(15)\n",
      "```\n",
      "\n",
      "In this function, `i % 3 == 0` checks if the number is divisible by 3, and `i % 5 == 0` checks if it's divisible by 5. Based on these conditions, it prints either \"Fizz\", \"Buzz\", or \"FizzBuzz\". If the number doesn't meet any of these conditions, it simply prints the number itself.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-nemo\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "Compared to the other two models discussed in this lesson, Mistral NeMo is the only free model with an Apache2 License. \n",
    "\n",
    "It is viewed as an upgrade to the earlier open source LLM from Mistral, Mistral 7B. \n",
    "\n",
    "Some other feature of the NeMo model are: \n",
    "\n",
    "- *More efficient tokenization:* This model using the Tekken tokenizer over the more commonly used tiktoken. This allows for better performance over more languages and code. \n",
    "\n",
    "- *Finetuning:* The base model is available for finetuning. This allows for more flexibility for use-cases where finetuning may be needed. \n",
    "\n",
    "- *Native Function Calling* - Like Mistral Large, this model has been trained on function calling. This makes it unique as being one of the first open source models to do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "Compared to the other two models discussed in this lesson, Mistral NeMo is the only free model with an Apache2 License. \n",
    "\n",
    "It is viewed as an upgrade to the earlier open source LLM from Mistral, Mistral 7B. \n",
    "\n",
    "Some other feature of the NeMo model are: \n",
    "\n",
    "- *More efficient tokenization:* This model uses the Tekken tokenizer over the more commonly used tiktoken. This allows for better performance over more languages and code. \n",
    "\n",
    "- *Finetuning:* The base model is available for finetuning. This allows for more flexibility for use-cases where finetuning may be needed. \n",
    "\n",
    "- *Native Function Calling* - Like Mistral Large, this model has been trained on function calling. This makes it unique as being one of the first open source models to do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Tokenizers \n",
    "\n",
    "In this sample, we will look at how Mistral NeMo handles tokenization compared to Mistral Large. \n",
    "\n",
    "Both samples take the same prompt but you shoud see that NeMo returns back less tokens vs Mistral Large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistral-common[sentencepiece] in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (1.8.5)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (2.12.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (4.25.1)\n",
      "Requirement already satisfied: typing-extensions>=4.11.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (4.15.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (0.12.0)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (12.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.25 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from mistral-common[sentencepiece]) (2.3.3)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common[sentencepiece]) (2.10.6)\n",
      "Collecting sentencepiece>=0.2.0 (from mistral-common[sentencepiece])\n",
      "  Using cached sentencepiece-0.2.1.tar.gz (3.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common[sentencepiece]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common[sentencepiece]) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from pydantic<3.0,>=2.7->mistral-common[sentencepiece]) (0.4.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common[sentencepiece]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common[sentencepiece]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common[sentencepiece]) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral-common[sentencepiece]) (0.28.0)\n",
      "Requirement already satisfied: pycountry>=23 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common[sentencepiece]) (24.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.0.0->mistral-common[sentencepiece]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.0.0->mistral-common[sentencepiece]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.0.0->mistral-common[sentencepiece]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.0.0->mistral-common[sentencepiece]) (2025.10.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from tiktoken>=0.7.0->mistral-common[sentencepiece]) (2025.10.23)\n",
      "Building wheels for collected packages: sentencepiece\n",
      "  Building wheel for sentencepiece (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for sentencepiece \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[92 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/__init__.py -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/_version.py -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece_model_pb2.py -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece_pb2.py -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing src/sentencepiece.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to src/sentencepiece.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to src/sentencepiece.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to src/sentencepiece.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'src/sentencepiece.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'src/sentencepiece.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece.i -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece_wrap.cxx -> build/lib.linux-x86_64-cpython-312/sentencepiece\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/sentencepiece/package_data\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/package_data/nfkc.bin -> build/lib.linux-x86_64-cpython-312/sentencepiece/package_data\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/package_data/nfkc_cf.bin -> build/lib.linux-x86_64-cpython-312/sentencepiece/package_data\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/package_data/nmt_nfkc.bin -> build/lib.linux-x86_64-cpython-312/sentencepiece/package_data\n",
      "  \u001b[31m   \u001b[0m copying src/sentencepiece/package_data/nmt_nfkc_cf.bin -> build/lib.linux-x86_64-cpython-312/sentencepiece/package_data\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Package sentencepiece was not found in the pkg-config search path.\n",
      "  \u001b[31m   \u001b[0m Perhaps you should add the directory containing `sentencepiece.pc'\n",
      "  \u001b[31m   \u001b[0m to the PKG_CONFIG_PATH environment variable\n",
      "  \u001b[31m   \u001b[0m Package 'sentencepiece' not found\n",
      "  \u001b[31m   \u001b[0m ./build_bundled.sh: line 21: cmake: not found\n",
      "  \u001b[31m   \u001b[0m ./build_bundled.sh: line 22: cmake: not found\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 280, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 435, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 423, in _build\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 404, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 235, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 115, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/command/bdist_wheel.py\", line 370, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 96, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 484, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-t3zxn5ha/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 510, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 95, in build_extension\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/subprocess.py\", line 413, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['./build_bundled.sh', '0.2.1']' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for sentencepiece\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build sentencepiece\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31m╰─>\u001b[0m sentencepiece\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install mistral-common\n",
    "%pip install mistral-common[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/mistral.py:213: FutureWarning: Calling `MistralTokenizer.from_model(..., strict=False)` is deprecated as it can lead to incorrect tokenizers. It is strongly recommended to use MistralTokenizer.from_model(..., strict=True)` which will become the default in `mistral_common=1.10.0`.If you are using `mistral_common` for open-sourced model weights, we recommend using `MistralTokenizer.from_file('<path/to/tokenizer/file>')` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`sentencepiece` is not installed. Please install it with `pip install mistral-common[sentencepiece]`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load Mistral tokenizer\u001b[39;00m\n\u001b[32m     14\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mmistral-large-latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m tokenizer = \u001b[43mMistralTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Tokenize a list of messages\u001b[39;00m\n\u001b[32m     19\u001b[39m tokenized = tokenizer.encode_chat_completion(\n\u001b[32m     20\u001b[39m     ChatCompletionRequest(\n\u001b[32m     21\u001b[39m         tools=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     )\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/mistral.py:226\u001b[39m, in \u001b[36mMistralTokenizer.from_model\u001b[39m\u001b[34m(cls, model, strict)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, tokenizer_cls \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME_TO_TOKENIZER_CLS.items():\n\u001b[32m    225\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model.lower():\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME_TO_TOKENIZER_CLS:\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TokenizerException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/mistral.py:179\u001b[39m, in \u001b[36mMistralTokenizer.v3\u001b[39m\u001b[34m(cls, is_tekken, is_mm)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     tokenizer_name = \u001b[33m\"\u001b[39m\u001b[33mmistral_instruct_tokenizer_240323.model.v3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mValidationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/mistral.py:290\u001b[39m, in \u001b[36mMistralTokenizer.from_file\u001b[39m\u001b[34m(cls, tokenizer_filename, mode)\u001b[39m\n\u001b[32m    288\u001b[39m     audio_config = tokenizer.audio\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_sentencepiece(tokenizer_filename):\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     tokenizer = \u001b[43mSentencePieceTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     image_config = get_image_config(tokenizer_filename)\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# spm can't have audio\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/sentencepiece.py:80\u001b[39m, in \u001b[36mSentencePieceTokenizer.__init__\u001b[39m\u001b[34m(self, model_path, tokenizer_version)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: Union[\u001b[38;5;28mstr\u001b[39m, Path], tokenizer_version: Optional[TokenizerVersion] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     74\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Initialize the `SentencePieceTokenizer`.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m        model_path: The path to the `SentencePiece` model.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m        tokenizer_version: The version of the tokenizer. If not provided, it will be inferred from the model path.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43massert_sentencepiece_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger = logging.getLogger(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# reload tokenizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/imports.py:72\u001b[39m, in \u001b[36massert_sentencepiece_installed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massert_sentencepiece_installed\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43massert_package_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dependency_error_message\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/mistral_common/imports.py:20\u001b[39m, in \u001b[36massert_package_installed\u001b[39m\u001b[34m(package_name, error_message)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_package_installed(package_name):\n\u001b[32m     19\u001b[39m     error_message = error_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPackage \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is required but not installed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(error_message)\n",
      "\u001b[31mImportError\u001b[39m: `sentencepiece` is not installed. Please install it with `pip install mistral-common[sentencepiece]`"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning does not stop here, continue the Journey\n",
    "\n",
    "After completing this lesson, check out our [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) to continue leveling up your Generative AI knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
